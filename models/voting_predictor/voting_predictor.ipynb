{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Predictor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import seed\n",
    "import os\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "method to create lagged features\n",
    "\n",
    "data - data\n",
    "to_keep - number of lagged_features\n",
    "to_remove - number of days to remove\n",
    "\n",
    "\"\"\"\n",
    "def create_lagged_features(data, to_keep=1, to_remove=1):\n",
    "    variables = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    columns, names = list(), list()\n",
    "    \n",
    "    for i in range(to_keep, 0, -1):\n",
    "        columns.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(variables)]\n",
    "\n",
    "    for i in range(0, to_remove):\n",
    "        columns.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(variables)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(variables)]\n",
    "            \n",
    "    #put it all together\n",
    "    final = concat(columns, axis=1)\n",
    "    final.columns = names\n",
    "    \n",
    "    #drop rows with NaN values\n",
    "    final.dropna(inplace=True)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to calculate rsi\n",
    "\n",
    "data - data\n",
    "period - RSI period\n",
    "\n",
    "\"\"\"\n",
    "def rsi(data, period: int = 14):\n",
    "    \n",
    "    delta = data[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = gain / loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(lag, lagged_features, type):\n",
    "    #lag granularity - days or hours\n",
    "    lag_granularity = \"days\"\n",
    "    # type of analyser - TextBlob or vader\n",
    "    analyser = \"vader\"\n",
    "    #dataset grouped type - day or hour\n",
    "    dataset_grouped_by = \"day\"\n",
    "    \n",
    "    #read dataset\n",
    "    folder = \"./../../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets/cleaned\"\n",
    "    filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #group by datetime\n",
    "    df = df.groupby('DateTime').agg(lambda x: x.mean())\n",
    "    \n",
    "    #get label\n",
    "    if(type == \"trend\"):\n",
    "        df[\"Change\"] = (df[\"Close\"] > df[\"Close\"].shift(1)).astype(int)\n",
    "        \n",
    "    elif(type == \"multiclass\"):\n",
    "        #calculate change\n",
    "        df[\"Change\"] = (df[\"Close\"] - df[\"Close\"].shift(1)).astype(float)\n",
    "        #drop empty\n",
    "        df = df.dropna(subset=['Change'])\n",
    "        #max positive change \n",
    "        max_change = df[\"Change\"].max()\n",
    "        #max negative change \n",
    "        min_change = df[\"Change\"].min()\n",
    "\n",
    "        #prepare bins\n",
    "        rnge = max_change - min_change\n",
    "        bin_size = (max_change - min_change) / 10\n",
    "        half_range = rnge/2\n",
    "        bins = np.arange(-1*half_range, half_range, bin_size)\n",
    "        bins[5] = 0\n",
    "        bins[0] = float(\"-inf\")\n",
    "        bins = np.append(bins, float(\"inf\"))\n",
    "        #more specific bins\n",
    "        bins = [float(\"-inf\"), -1320, -990, -660, -330, 0., 330, 660, 990, 1320, float(\"inf\")]\n",
    "        labels = [0, 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "        #set bins\n",
    "        df['Change'] = pd.cut(x=df['Change'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    add_RSI = False\n",
    "    add_longMAvg = False\n",
    "    add_shortMAvg = False\n",
    "\n",
    "    if(add_RSI):\n",
    "        #calcualte RSI\n",
    "        RSI = 14\n",
    "        df['RSI'] = rsi(df, RSI)\n",
    "        df = df.iloc[RSI:]\n",
    "\n",
    "    #calculate moving averages\n",
    "    if(add_shortMAvg):\n",
    "        short_window = 9\n",
    "        df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        long_window = 21\n",
    "        df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        df = df.iloc[long_window:]\n",
    "    elif(add_RSI):\n",
    "        df = df.iloc[RSI:]\n",
    "    elif(add_shortMAvg):\n",
    "        df = df.iloc[short_window:]\n",
    "        \n",
    "    #keep only wanted columns\n",
    "    features = ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "    if(add_RSI):\n",
    "        features.append(\"RSI\")\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        features.append(\"long_mavg\")\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        features.append(\"short_mavg\")\n",
    "\n",
    "    df = df[features]\n",
    "    \n",
    "    #number of previous records to consider for every example\n",
    "    n_lag = lagged_features\n",
    "    #number of features\n",
    "    n_features = len(features)\n",
    "    #calculate total_features\n",
    "    total_features = n_lag*n_features\n",
    "\n",
    "    if(total_features == 0):\n",
    "        total_features = n_features\n",
    "        \n",
    "    #add lagged data to records\n",
    "    data_with_lagged = create_lagged_features(df, n_lag, 1)\n",
    "    data_with_lagged = data_with_lagged.reset_index()\n",
    "    \n",
    "    return data_with_lagged, total_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(data, total_features, type):\n",
    "    \n",
    "    data = data.drop(['DateTime'], axis=1)\n",
    "    \n",
    "    if(type == \"trend\"):\n",
    "        scaler = joblib.load(\"../bilstm_trend/saved/scaler.pkl\")\n",
    "    elif(type == \"multiclass\"):\n",
    "        scaler = joblib.load(\"../cnn_multiclass/saved/scaler.pkl\") \n",
    "        \n",
    "    data_y = data[\"var1(t)\"].values\n",
    "    data = scaler.transform(data)\n",
    "    data_X = data[:, :total_features]\n",
    "    \n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, seed):\n",
    "    #shuffle data\n",
    "    np.random.seed(seed)\n",
    "    #shuffle times \n",
    "    data = shuffle(data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_lag = 3\n",
    "multiclass_lagged_features = 3\n",
    "\n",
    "trend_lag = 1\n",
    "trend_lagged_features = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../../datasets/tweets_prices_volumes_sentiment/vader/day_datasets/cleaned/final_data_lag_days_3.csv\n",
      "./../../datasets/tweets_prices_volumes_sentiment/vader/day_datasets/cleaned/final_data_lag_days_1.csv\n"
     ]
    }
   ],
   "source": [
    "#load data for multiclass\n",
    "data_multiclass, data_multiclass_total_features = load_data(multiclass_lag, multiclass_lagged_features, \"multiclass\")\n",
    "#load data for trend\n",
    "data_trend, data_trend_total_features = load_data(trend_lag, trend_lagged_features, \"trend\")\n",
    "\n",
    "#get dates of lag1\n",
    "data_trend_dates = data_trend[\"DateTime\"].unique()\n",
    "#get dates of lag3\n",
    "data_multiclass_dates = data_multiclass[\"DateTime\"].unique()\n",
    "\n",
    "#get common dates\n",
    "common_dates = list(set(data_trend_dates).intersection(data_multiclass_dates))\n",
    "#keep only common dates\n",
    "data_multiclass = data_multiclass.loc[data_multiclass['DateTime'].isin(common_dates)]\n",
    "data_trend = data_trend.loc[data_trend['DateTime'].isin(common_dates)]\n",
    "\n",
    "#split into features and labels\n",
    "data_multiclass_X, data_multiclass_y = get_features_labels(data_multiclass, data_multiclass_total_features, \"multiclass\")\n",
    "data_trend_X, data_trend_y = get_features_labels(data_trend, data_trend_total_features, \"trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.34567324, 0.13516987, 0.3342473 , 0.00192975,\n",
       "       0.375     , 0.34359594, 0.16142688, 0.23499127, 0.00194518,\n",
       "       0.5       , 0.3181856 , 0.19456161, 0.39475941, 0.00111157])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove last 2 records to match\n",
    "data_multiclass_X = data_multiclass_X[:-2]\n",
    "data_multiclass_y = data_multiclass_y[:-2]\n",
    "data_multiclass_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.43532498, 0.16350344, 0.18778724, 0.00150193,\n",
       "       1.        , 0.43721813, 0.16403497, 0.33764505, 0.00153822,\n",
       "       1.        , 0.44403559, 0.15568921, 0.33026542, 0.00195018,\n",
       "       0.        , 0.398254  , 0.18696129, 0.20720317, 0.00219157,\n",
       "       0.        , 0.34567324, 0.208892  , 0.4000871 , 0.00245205,\n",
       "       0.        , 0.34359594, 0.20369029, 0.11478021, 0.00232019,\n",
       "       0.        , 0.3181856 , 0.21991289, 0.30004474, 0.00122954])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove first two records\n",
    "data_trend_X = data_trend_X[2:]\n",
    "data_trend_y = data_trend_y[2:]\n",
    "data_trend_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "data_multiclass_X = shuffle_data(data_multiclass_X, 1)\n",
    "data_multiclass_y = shuffle_data(data_multiclass_y, 1)\n",
    "data_trend_X = shuffle_data(data_trend_X, 1)\n",
    "data_trend_y = shuffle_data(data_trend_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55555556, 0.33400191, 0.64269957, 0.364364  , 0.00418369,\n",
       "       0.75      , 0.41511056, 0.54471334, 0.693398  , 0.00568117,\n",
       "       0.625     , 0.42242223, 0.57105899, 0.41363757, 0.00541892])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_multiclass_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.26605875, 0.66137848, 0.30046663, 0.00370017,\n",
       "       1.        , 0.28513863, 0.63940156, 0.2904628 , 0.0041465 ,\n",
       "       1.        , 0.28875371, 0.59241676, 0.35919249, 0.00435832,\n",
       "       1.        , 0.31142931, 0.5535872 , 0.68698868, 0.00602425,\n",
       "       1.        , 0.33471749, 0.58344316, 0.41552758, 0.00625101,\n",
       "       1.        , 0.41511056, 0.64341905, 0.71567104, 0.25352272,\n",
       "       1.        , 0.42242223, 0.67025069, 0.64910532, 0.47876078])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trend_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trend_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 3, 4, 5, ..., 4, 3, 5, 5, 5]\n",
       "Length: 442\n",
       "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_multiclass_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only last few\n",
    "elements_to_keep = 200\n",
    "\n",
    "data_multiclass_X = data_multiclass_X[-1*elements_to_keep:]\n",
    "data_multiclass_y = data_multiclass_y[-1*elements_to_keep:]\n",
    "data_trend_X = data_trend_X[-1*elements_to_keep:]\n",
    "data_trend_y = data_trend_y[-1*elements_to_keep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get trend model\n",
    "trend_model = load_model(\"../bilstm_trend/saved/ckpt\")\n",
    "\n",
    "#get multiclass model\n",
    "multiclass_model = load_model(\"../cnn_multiclass/saved/ckpt\")\n",
    "\n",
    "#get normalizer\n",
    "# trend_scaler = joblib.load(\"../bilstm_trend/saved/scaler.pkl\")\n",
    "# mutliclass_scaler = joblib.load(\"../cnn_multiclass/saved/scaler.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise features\n",
    "# data_multiclass_X = mutliclass_scaler.transform(data_multiclass_X)\n",
    "# data_trend_X = trend_scaler.transform(data_trend_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "multiclass_features = int(data_multiclass_total_features/multiclass_lagged_features)\n",
    "data_multiclass_X = data_multiclass_X.reshape((data_multiclass_X.shape[0], multiclass_lagged_features, multiclass_features))\n",
    "trend_features = int(data_trend_total_features/trend_lagged_features)\n",
    "data_trend_X = data_trend_X.reshape((data_trend_X.shape[0], trend_lagged_features, trend_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "trend_pred = trend_model.predict(data_trend_X)\n",
    "trend_pred = np.argmax(trend_pred, axis=1)\n",
    "\n",
    "multiclass_pred = multiclass_model.predict(data_multiclass_X)\n",
    "multiclass_pred = np.argmax(multiclass_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "preds_len = len(trend_pred)\n",
    "for i in range (0, preds_len):\n",
    "    if(trend_pred[i] == 0 and multiclass_pred[i] < 5):\n",
    "        pred.append(0)\n",
    "    elif(trend_pred[i] == 1 and multiclass_pred[i] > 4):\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(-1)\n",
    "\n",
    "uncertain_indices = [index for index,value in enumerate(pred) if value == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted indices\n",
    "pred = [value for index,value in enumerate(pred) if index not in uncertain_indices]\n",
    "data_trend_y = [value for index,value in enumerate(data_trend_y) if index not in uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 68\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\",len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6176470588235294\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "prices = pd.DataFrame()\n",
    "prices[\"Actual\"] = data_trend_y\n",
    "prices[\"Predicted\"] = pred\n",
    "\n",
    "prices[\"Correct\"] = (prices[\"Actual\"] - prices[\"Predicted\"]) == 0\n",
    "incorrect = prices.loc[prices['Correct'] == False]\n",
    "incorrect_len = len(incorrect)\n",
    "prices_len = len(prices)\n",
    "\n",
    "print(\"Accuracy = \",((prices_len-incorrect_len)/prices_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(elements_to_keep, shuffle_seed):\n",
    "    #load data for multiclass\n",
    "    data_multiclass, data_multiclass_total_features = load_data(multiclass_lag, multiclass_lagged_features, \"multiclass\")\n",
    "    #load data for trend\n",
    "    data_trend, data_trend_total_features = load_data(trend_lag, trend_lagged_features, \"trend\")\n",
    "\n",
    "    #get dates of lag1\n",
    "    data_trend_dates = data_trend[\"DateTime\"].unique()\n",
    "    #get dates of lag3\n",
    "    data_multiclass_dates = data_multiclass[\"DateTime\"].unique()\n",
    "\n",
    "    #get common dates\n",
    "    common_dates = list(set(data_trend_dates).intersection(data_multiclass_dates))\n",
    "    #keep only common dates\n",
    "    data_multiclass = data_multiclass.loc[data_multiclass['DateTime'].isin(common_dates)]\n",
    "    data_trend = data_trend.loc[data_trend['DateTime'].isin(common_dates)]\n",
    "\n",
    "    #split into features and labels\n",
    "    data_multiclass_X, data_multiclass_y = get_features_labels(data_multiclass, data_multiclass_total_features, \"multiclass\")\n",
    "    data_trend_X, data_trend_y = get_features_labels(data_trend, data_trend_total_features, \"trend\")\n",
    "    \n",
    "    #remove last 2 records to match\n",
    "    data_multiclass_X = data_multiclass_X[:-2]\n",
    "    data_multiclass_y = data_multiclass_y[:-2]\n",
    "\n",
    "    #remove first two records\n",
    "    data_trend_X = data_trend_X[2:]\n",
    "    data_trend_y = data_trend_y[2:]\n",
    "\n",
    "    #shuffle\n",
    "    data_multiclass_X = shuffle_data(data_multiclass_X, shuffle_seed)\n",
    "    data_multiclass_y = shuffle_data(data_multiclass_y, shuffle_seed)\n",
    "    data_trend_X = shuffle_data(data_trend_X, shuffle_seed)\n",
    "    data_trend_y = shuffle_data(data_trend_y, shuffle_seed)\n",
    "\n",
    "    data_multiclass_X = data_multiclass_X[-1*elements_to_keep:]\n",
    "    data_multiclass_y = data_multiclass_y[-1*elements_to_keep:]\n",
    "    data_trend_X = data_trend_X[-1*elements_to_keep:]\n",
    "    data_trend_y = data_trend_y[-1*elements_to_keep:]\n",
    "\n",
    "    #get trend model\n",
    "    trend_model = load_model(\"../bilstm_trend/saved/ckpt\")\n",
    "\n",
    "    #get multiclass model\n",
    "    multiclass_model = load_model(\"../cnn_multiclass/saved/ckpt\")\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    multiclass_features = int(data_multiclass_total_features/multiclass_lagged_features)\n",
    "    data_multiclass_X = data_multiclass_X.reshape((data_multiclass_X.shape[0], multiclass_lagged_features, multiclass_features))\n",
    "    trend_features = int(data_trend_total_features/trend_lagged_features)\n",
    "    data_trend_X = data_trend_X.reshape((data_trend_X.shape[0], trend_lagged_features, trend_features))\n",
    "    \n",
    "    #make prediction\n",
    "    trend_pred = trend_model.predict(data_trend_X)\n",
    "    trend_pred = np.argmax(trend_pred, axis=1)\n",
    "\n",
    "    multiclass_pred = multiclass_model.predict(data_multiclass_X)\n",
    "    multiclass_pred = np.argmax(multiclass_pred, axis=1)\n",
    "    \n",
    "    pred = []\n",
    "    preds_len = len(trend_pred)\n",
    "    for i in range (0, preds_len):\n",
    "        if(trend_pred[i] == 0 and multiclass_pred[i] < 5):\n",
    "            pred.append(0)\n",
    "        elif(trend_pred[i] == 1 and multiclass_pred[i] > 4):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(-1)\n",
    "\n",
    "    uncertain_indices = [index for index,value in enumerate(pred) if value == -1]\n",
    "    \n",
    "    #remove unwanted indices\n",
    "    pred = [value for index,value in enumerate(pred) if index not in uncertain_indices]\n",
    "    data_trend_y = [value for index,value in enumerate(data_trend_y) if index not in uncertain_indices]\n",
    "    \n",
    "    #calculate accuracy\n",
    "    prices = pd.DataFrame()\n",
    "    prices[\"Actual\"] = data_trend_y\n",
    "    prices[\"Predicted\"] = pred\n",
    "\n",
    "    prices[\"Correct\"] = (prices[\"Actual\"] - prices[\"Predicted\"]) == 0\n",
    "    incorrect = prices.loc[prices['Correct'] == False]\n",
    "    incorrect_len = len(incorrect)\n",
    "    prices_len = len(prices)\n",
    "\n",
    "    print(\"Accuracy = \",((prices_len-incorrect_len)/prices_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../../datasets/tweets_prices_volumes_sentiment/vader/day_datasets/cleaned/final_data_lag_days_3.csv\n",
      "./../../datasets/tweets_prices_volumes_sentiment/vader/day_datasets/cleaned/final_data_lag_days_1.csv\n",
      "Accuracy =  0.6176470588235294\n"
     ]
    }
   ],
   "source": [
    "run_prediction(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
