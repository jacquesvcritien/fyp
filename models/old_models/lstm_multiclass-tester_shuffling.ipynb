{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test the paramters of the LSTM trend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "import sklearn\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "# from keras.layers import CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "import keras.utils\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import seed\n",
    "import os\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate rsi\n",
    "def rsi(ohlc, period: int = 14) -> pd.Series:\n",
    "    \"\"\"See source https://github.com/peerchemist/finta\n",
    "    and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n",
    "    Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements.\n",
    "    RSI oscillates between zero and 100. Traditionally, and according to Wilder, RSI is considered overbought when above 70 and oversold when below 30.\n",
    "    Signals can also be generated by looking for divergences, failure swings and centerline crossovers.\n",
    "    RSI can also be used to identify the general trend.\"\"\"\n",
    "\n",
    "    delta = ohlc[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    _gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    _loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = _gain / _loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_to_rem_label(n_lag, n_features, label_col):\n",
    "    to_rem = []\n",
    "    for i in range(1, n_features+1):\n",
    "        for j in range(0, n_lag+1):\n",
    "            if(j!=0 or i != label_col):\n",
    "                if(j == 0):\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t)\")\n",
    "                else:\n",
    "                    to_rem.append(\"var\"+str(i)+\"(t-\"+str(j)+\")\")\n",
    "    return to_rem\n",
    "    \n",
    "def get_cols_to_rem_feat(n_lag, n_features, label_col):\n",
    "    #uncomment the below line to only remove the last price\n",
    "    to_rem = [\"var1(t)\", \"var2(t)\"]\n",
    "    return to_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lag_sets(step, lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned):\n",
    "    # type of analyser - TextBlob or vader\n",
    "    analyser = \"vader\"\n",
    "    # analyser = \"TextBlob\"\n",
    "    \n",
    "    #read dataset\n",
    "    folder = \"./../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets\"\n",
    "    \n",
    "    if cleaned:\n",
    "        folder = folder + '/cleaned'\n",
    "        \n",
    "    filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #group by datetime\n",
    "    df = df.groupby('DateTime').agg(lambda x: x.mean())\n",
    "    \n",
    "    #calculate change\n",
    "    df[\"Change\"] = (df[\"Close\"] - df[\"Close\"].shift(1)).astype(float)\n",
    "    #drop empty\n",
    "    df = df.dropna(subset=['Change'])\n",
    "    #max positive change \n",
    "    max_change = df[\"Change\"].max()\n",
    "    #max negative change \n",
    "    min_change = df[\"Change\"].min()\n",
    "    \n",
    "    #prepare bins\n",
    "    rnge = max_change - min_change\n",
    "    bin_size = (max_change - min_change) / 10\n",
    "    half_range = rnge/2\n",
    "    bins = np.arange(-1*half_range, half_range, bin_size)\n",
    "    bins[5] = 0\n",
    "    bins[0] = float(\"-inf\")\n",
    "    bins = np.append(bins, float(\"inf\"))\n",
    "    labels = [0, 1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    #set bins\n",
    "    df['Change'] = pd.cut(x=df['Change'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    add_RSI = False\n",
    "    add_longMAvg = False\n",
    "    add_shortMAvg = False\n",
    "\n",
    "    if(add_RSI):\n",
    "        #calcualte RSI\n",
    "        RSI = 14\n",
    "        df['RSI'] = rsi(df, RSI)\n",
    "        df = df.iloc[RSI:]\n",
    "\n",
    "    #calcualte moving averages\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        short_window = 9\n",
    "        df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        long_window = 21\n",
    "        df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        df = df.iloc[long_window:]\n",
    "    elif(add_RSI):\n",
    "        df = df.iloc[RSI:]\n",
    "    elif(add_shortMAvg):\n",
    "        df = df.iloc[short_window:]\n",
    "        \n",
    "        \n",
    "    #keep only wanted columns\n",
    "    features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "    # features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'compound', 'pos_pol', 'neg_pol', 'neu_pol', 'Tweet_vol','Volume_(BTC)']\n",
    "\n",
    "    if(add_RSI):\n",
    "        features.append(\"RSI\")\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        features.append(\"long_mavg\")\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        features.append(\"short_mavg\")\n",
    "\n",
    "    df = df[features]\n",
    "\n",
    "    #creating copy so that data is not loaded once again\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    #number of previous records to consider for every example\n",
    "    n_lag = lag_features\n",
    "    #number of features\n",
    "    n_features = len(features)\n",
    "    #calcualte total_features\n",
    "    total_features = n_lag*n_features\n",
    "\n",
    "    if(total_features == 0):\n",
    "        total_features = n_features\n",
    "        \n",
    "    df_reframed = series_to_supervised(df, n_lag, 1)\n",
    "    df_reframed =  df_reframed.reset_index()\n",
    "    df_reframed = df_reframed.drop(['DateTime'], axis=1)\n",
    "    \n",
    "    #shuffle\n",
    "    np.random.seed(1)\n",
    "    for j in range(0, step+1):\n",
    "        df_reframed = shuffle(df_reframed)\n",
    "    \n",
    "    #divide df into train and test\n",
    "    data_len = len(df_reframed)\n",
    "    train_size = int(data_len*train_ratio)\n",
    "\n",
    "    train = df_reframed.iloc[:train_size]\n",
    "    test = df_reframed.iloc[train_size:]\n",
    "    \n",
    "    train_y = train[\"var1(t)\"].values\n",
    "    test_y = test[\"var1(t)\"].values\n",
    "\n",
    "#     train_y = train_y.reshape(len(train_y), 1)\n",
    "#     test_y = test_y.reshape(len(test_y), 1)\n",
    "    \n",
    "    train_reframed = train\n",
    "    test_reframed = test\n",
    "    \n",
    "    train_reframed = train_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)\n",
    "    test_reframed = test_reframed.drop(get_cols_to_rem_feat(n_lag, n_features, 1), axis=1)\n",
    "    \n",
    "    #normalise features\n",
    "    xscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_reframed = xscaler.fit_transform(train_reframed)\n",
    "    test_reframed = xscaler.transform(test_reframed)\n",
    "\n",
    "    train = train_reframed\n",
    "    test = test_reframed\n",
    "    train_labels = train_y\n",
    "    test_labels = test_y\n",
    "\n",
    "    train = train_reframed\n",
    "    test = test_reframed\n",
    "    \n",
    "    #remove the last set of values(data of time to be predicted)\n",
    "    train = train[:, :total_features]\n",
    "    test = test[:, :total_features]\n",
    "\n",
    "    #keep only prices array\n",
    "    train_X, train_y = train[:, :total_features], train_y\n",
    "    test_X, test_y = test[:, :total_features], test_y\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    if(n_lag > 0):\n",
    "        train_X = train_X.reshape((train_X.shape[0], n_lag, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "    else:\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, n_features-1))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, n_features-1))\n",
    "\n",
    "    train_y = keras.utils.to_categorical(train_y, 10)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y, len(features), df, train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_test(epochs, neurons, batch_size, layers, train_X, test_X, train_y, test_y, lag_features, features, df, train_size):\n",
    "    global n_lag\n",
    "    global n_features\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    dropout = 0.25\n",
    "    activ_func = \"linear\"\n",
    "    \n",
    "    return_seq = layers > 1\n",
    "\n",
    "    model.add(LSTM(neurons, return_sequences=return_seq, input_shape=(train_X.shape[1], train_X.shape[2]), activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        ret_seq = i != (layers-1)\n",
    "        model.add(LSTM(neurons, return_sequences=ret_seq, activation=activ_func))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,validation_split=0.2)\n",
    "    \n",
    "    if(lag_features > 0):\n",
    "        test_X = test_X.reshape((test_X.shape[0], lag_features, features))\n",
    "    else:\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, features-1))\n",
    "\n",
    "    pred = model.predict(test_X)\n",
    "\n",
    "    if(lag_features > 0):\n",
    "        test_X = test_X.reshape((test_X.shape[0], lag_features* features,))\n",
    "    else:\n",
    "        test_X = test_X.reshape((test_X.shape[0], features-1,))\n",
    "        \n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    report = sklearn.metrics.classification_report(test_y, pred, zero_division=0,output_dict=True)\n",
    "    \n",
    "    #obtain f1-scores for classes\n",
    "    f1 = np.zeros(10)\n",
    "    for i in range(0,10):\n",
    "        f1[i] = report[str(i)]['f1-score'] if str(i) in report else 0\n",
    "        \n",
    "    #return accuracy and f1\n",
    "    return report['accuracy'], f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(lag_granularity, lag, dataset_grouped_by, cleaned):\n",
    "    #get filename\n",
    "    filename = 'results/lstm_multiclass/lstm_multiclass_groupedby_'+dataset_grouped_by+\"_lag_\"+lag_granularity+\"_\"+str(lag)\n",
    "\n",
    "    if cleaned:\n",
    "        filename = filename + '_cleaned'\n",
    "\n",
    "    full_filename = filename+\".csv\"\n",
    "\n",
    "    columns = [\"lag\", \"epochs\", \"batch_size\", \"neurons\", \"layers\", \"split\", \"accuracy\", \"f1-0\", \"f1-1\", \"f1-2\", \"f1-3\", \"f1-4\", \"f1-5\", \"f1-6\", \"f1-7\", \"f1-8\", \"f1-9\"]\n",
    "    try:\n",
    "        results = pd.read_csv(full_filename)\n",
    "    except:\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "    lags = [3]\n",
    "    train_ratio = 0.85\n",
    "    \n",
    "    for lag_features in lags:\n",
    "\n",
    "        epochs = [50, 200, 500, 1000, 2000]\n",
    "        neurons = [32, 128, 256]\n",
    "        layers = [1, 2, 3]\n",
    "        batch_sizes = [5, 20, 50, 80]\n",
    "        \n",
    "#         epochs = [50]\n",
    "#         neurons = [32]\n",
    "#         layers = [1]\n",
    "#         batch_sizes = [500]\n",
    "\n",
    "        for e in epochs:\n",
    "            for n in neurons:\n",
    "                for l in layers:\n",
    "                    for b in  batch_sizes:\n",
    "                        accuracies = []\n",
    "                        f1_0 = []\n",
    "                        f1_1 = []\n",
    "                        f1_2 = []\n",
    "                        f1_3 = []\n",
    "                        f1_4 = []\n",
    "                        f1_5 = []\n",
    "                        f1_6 = []\n",
    "                        f1_7 = []\n",
    "                        f1_8 = []\n",
    "                        f1_9 = []\n",
    "                        print(\"Testing model: lag:\", lag_features, \", epochs:\", e, \", neurons:\", n, \", layers:\", l, \", batch_size:\", b)\n",
    "                        \n",
    "                        for i in range (0,5):\n",
    "                            train_X, test_X, train_y, test_y, features, df, train_size = load_lag_sets(i, lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned)\n",
    "                            acc, f1 = create_model_test(e, n, b, l, train_X, test_X, train_y, test_y, lag_features, features, df, train_size)\n",
    "                            accuracies.append(acc)\n",
    "                            f1_0.append(f1[0])\n",
    "                            f1_1.append(f1[1])\n",
    "                            f1_2.append(f1[2])\n",
    "                            f1_3.append(f1[3])\n",
    "                            f1_4.append(f1[4])\n",
    "                            f1_5.append(f1[5])\n",
    "                            f1_6.append(f1[6])\n",
    "                            f1_7.append(f1[7])\n",
    "                            f1_8.append(f1[8])\n",
    "                            f1_9.append(f1[9])\n",
    "                            \n",
    "                        \n",
    "                        accuracies = np.array(accuracies)\n",
    "                        f1_0 = np.array(f1_0)\n",
    "                        f1_1 = np.array(f1_1)\n",
    "                        f1_2 = np.array(f1_2)\n",
    "                        f1_3 = np.array(f1_3)\n",
    "                        f1_4 = np.array(f1_4)\n",
    "                        f1_5 = np.array(f1_5)\n",
    "                        f1_6 = np.array(f1_6)\n",
    "                        f1_7 = np.array(f1_7)\n",
    "                        f1_8 = np.array(f1_8)\n",
    "                        f1_9 = np.array(f1_9)\n",
    "                        \n",
    "                        mean_acc =accuracies.mean()\n",
    "                        min_acc =accuracies.min()\n",
    "                        max_acc =accuracies.max()\n",
    "                        diff_acc = max_acc - min_acc\n",
    "                        \n",
    "                        mean_f1_0 = f1_0.mean();\n",
    "                        mean_f1_1 = f1_1.mean();\n",
    "                        mean_f1_2 = f1_2.mean();\n",
    "                        mean_f1_3 = f1_3.mean();\n",
    "                        mean_f1_4 = f1_4.mean();\n",
    "                        mean_f1_5 = f1_5.mean();\n",
    "                        mean_f1_6 = f1_6.mean();\n",
    "                        mean_f1_7 = f1_7.mean();\n",
    "                        mean_f1_8 = f1_8.mean();\n",
    "                        mean_f1_9 = f1_9.mean();\n",
    "                        \n",
    "                        results = results.append({\"lag\": lag_features, \"epochs\": e, \"batch_size\": b, \"neurons\":n, \"layers\":l, \"split\": train_ratio, \"mean_acc\": mean_acc, \"min_acc\": min_acc, \"max_acc\": max_acc, \"diff_acc\":diff_acc, \"f1-0\": mean_f1_0, \"f1-1\": mean_f1_1, \"f1-2\": mean_f1_2, \"f1-3\": mean_f1_3, \"f1-4\": mean_f1_4, \"f1-5\": mean_f1_5, \"f1-6\": mean_f1_6, \"f1-7\": mean_f1_7, \"f1-8\": mean_f1_8, \"f1-9\": mean_f1_9}, ignore_index=True)\n",
    "                \n",
    "    return results, full_filename\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag granularity - days or hours\n",
    "lag_granularity = \"days\"\n",
    "#lag value\n",
    "lag = 1\n",
    "#dataset grouped type - day or hour\n",
    "dataset_grouped_by = \"day\"\n",
    "#cleaned\n",
    "cleaned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 32 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 128 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 50 , neurons: 256 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 32 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 128 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 200 , neurons: 256 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 32 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 128 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 2 , batch_size: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 500 , neurons: 256 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 32 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 128 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 3 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 3 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 3 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 1000 , neurons: 256 , layers: 3 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 1 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 1 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 1 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 1 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 2 , batch_size: 5\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 2 , batch_size: 20\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 2 , batch_size: 50\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 2 , batch_size: 80\n",
      "Testing model: lag: 3 , epochs: 2000 , neurons: 32 , layers: 3 , batch_size: 5\n"
     ]
    }
   ],
   "source": [
    "results, full_filename = implement(lag_granularity, lag, dataset_grouped_by, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>neurons</th>\n",
       "      <th>layers</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1-0</th>\n",
       "      <th>f1-1</th>\n",
       "      <th>f1-2</th>\n",
       "      <th>...</th>\n",
       "      <th>f1-4</th>\n",
       "      <th>f1-5</th>\n",
       "      <th>f1-6</th>\n",
       "      <th>f1-7</th>\n",
       "      <th>f1-8</th>\n",
       "      <th>f1-9</th>\n",
       "      <th>diff_acc</th>\n",
       "      <th>max_acc</th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>min_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.603266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.397059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101333</td>\n",
       "      <td>0.544526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101099</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.420588</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.444118</td>\n",
       "      <td>0.397059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444118</td>\n",
       "      <td>0.397059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414110</td>\n",
       "      <td>0.527565</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.420588</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446486</td>\n",
       "      <td>0.491932</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415333</td>\n",
       "      <td>0.465263</td>\n",
       "      <td>0.175253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.308824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390143</td>\n",
       "      <td>0.468617</td>\n",
       "      <td>0.128889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.279412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396816</td>\n",
       "      <td>0.482679</td>\n",
       "      <td>0.180808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>0.279412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lag  epochs  batch_size  neurons  layers  split  accuracy  f1-0  f1-1  \\\n",
       "0    1.0    50.0         5.0     32.0     1.0   0.85       NaN   0.0   0.0   \n",
       "1    1.0    50.0        20.0     32.0     1.0   0.85       NaN   0.0   0.0   \n",
       "2    1.0    50.0        50.0     32.0     1.0   0.85       NaN   0.0   0.0   \n",
       "3    1.0    50.0        80.0     32.0     1.0   0.85       NaN   0.0   0.0   \n",
       "4    1.0    50.0         5.0     32.0     2.0   0.85       NaN   0.0   0.0   \n",
       "..   ...     ...         ...      ...     ...    ...       ...   ...   ...   \n",
       "175  1.0  2000.0        80.0    256.0     2.0   0.85       NaN   0.0   0.0   \n",
       "176  1.0  2000.0         5.0    256.0     3.0   0.85       NaN   0.0   0.0   \n",
       "177  1.0  2000.0        20.0    256.0     3.0   0.85       NaN   0.0   0.0   \n",
       "178  1.0  2000.0        50.0    256.0     3.0   0.85       NaN   0.0   0.0   \n",
       "179  1.0  2000.0        80.0    256.0     3.0   0.85       NaN   0.0   0.0   \n",
       "\n",
       "         f1-2  ...      f1-4      f1-5      f1-6  f1-7  f1-8  f1-9  diff_acc  \\\n",
       "0    0.000000  ...  0.063333  0.603266  0.000000   0.0   0.0   0.0  0.102941   \n",
       "1    0.000000  ...  0.101333  0.544526  0.000000   0.0   0.0   0.0  0.117647   \n",
       "2    0.000000  ...  0.101099  0.488782  0.000000   0.0   0.0   0.0  0.176471   \n",
       "3    0.000000  ...  0.083721  0.598782  0.000000   0.0   0.0   0.0  0.117647   \n",
       "4    0.000000  ...  0.000000  0.615513  0.000000   0.0   0.0   0.0  0.102941   \n",
       "..        ...  ...       ...       ...       ...   ...   ...   ...       ...   \n",
       "175  0.200000  ...  0.414110  0.527565  0.190000   0.0   0.0   0.0  0.161765   \n",
       "176  0.000000  ...  0.446486  0.491932  0.166667   0.0   0.0   0.0  0.176471   \n",
       "177  0.000000  ...  0.415333  0.465263  0.175253   0.0   0.0   0.0  0.132353   \n",
       "178  0.200000  ...  0.390143  0.468617  0.128889   0.0   0.0   0.0  0.176471   \n",
       "179  0.133333  ...  0.396816  0.482679  0.180808   0.0   0.0   0.0  0.176471   \n",
       "\n",
       "      max_acc  mean_acc   min_acc  \n",
       "0    0.500000  0.435294  0.397059  \n",
       "1    0.500000  0.426471  0.382353  \n",
       "2    0.514706  0.420588  0.338235  \n",
       "3    0.514706  0.444118  0.397059  \n",
       "4    0.500000  0.444118  0.397059  \n",
       "..        ...       ...       ...  \n",
       "175  0.514706  0.420588  0.352941  \n",
       "176  0.514706  0.417647  0.338235  \n",
       "177  0.441176  0.388235  0.308824  \n",
       "178  0.455882  0.382353  0.279412  \n",
       "179  0.455882  0.394118  0.279412  \n",
       "\n",
       "[180 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(full_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
